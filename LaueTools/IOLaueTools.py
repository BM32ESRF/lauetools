#! python
"""
module of lauetools project

https://gitlab.esrf.fr/micha/lauetools/

JS Micha   Feb 2022

this module gathers functions to read and write ASCII file corresponding
to various data
"""
from __future__ import division
import os
import sys
import time
import string
import copy
from copy import deepcopy
import re
import io

import numpy as np

np.set_printoptions(precision=15)

if sys.version_info.major == 3:
    from LaueTools.dict_LaueTools import CST_ENERGYKEV, CCD_CALIBRATION_PARAMETERS
    PYTHON3 = True
    #print('-- OK! You are using python 3')
else:
    from dict_LaueTools import CST_ENERGYKEV, CCD_CALIBRATION_PARAMETERS
    PYTHON3 = False
    #print('-- OK! you are using python 2 but you would better install python 3')

DEFAULT_CCDLABEL = 'sCMOS'

# --- ------------  PROCEDURES
def writefile_cor(prefixfilename:str, twicetheta:list, chi:list, data_x:list, data_y:list, dataintensity:list,
                                            param: list=None,
                                            initialfilename:str=None,
                                            comments:str=None,
                                            sortedexit:bool=False,
                                            overwrite:int=1,
                                            data_sat=None,
                                            data_props=None,
                                            rectpix:int=0,  # RECTPIX
                                            dirname_output:str=None,
                                            verbose:int=0,                                       dict_data_spotsproperties=None
                                            ):
    """
    Write .cor file containing data
    one line   of header
    next lines of data
    last lines of comments

    + comments at the end for calibration CCD parameters that have been used for calculating
    2theta and chi for each peak (in addition to X,Y pixel position)

    :return: str, outputfilename (not including full path)

    :param sortedexit: bool, True then sort peaks by intensity for the outputfile. False, then do not sort (e.g. sorting already done in input .dat file . see dataintensity input parameter)

    :param overwrite: 1 overwrite existing file
                        0 write a file with '_new' added in the name

    :param initialfilename: original .dat file from which the created .cor is based 

    :param rectpix:   to deal with non squared pixel: ypixelsize = xpixelsize * (1.0 + rectpix)

    :param data_props: [array of dataproperties, list columns name]  (ie  peak sizes, pixdev etc...  see .dat file)

    :param param: list or dictionnary (5 or 6 elements) of detector geometry parameters (order and key is found in CCD_CALIBRATION_PARAMETERS)

    :param dict_data_spotsproperties: write additional columns from orignal peaksearch-based (.dat) spots properties.
    dict must have keys: 'columnsname' and 'data_spotsproperties'

    if data_sat list, add column to .cor file to mark saturated peaks
    """
    nbspots = len(twicetheta)

    outputfilename = prefixfilename + ".cor"

    if dirname_output is None:
        print('in writefile_cor: dirname_output is None so prefixfilename should contain folder', prefixfilename)
        fullpath = outputfilename
    else:
        fullpath = os.path.join(dirname_output, outputfilename)

    folder, filename  = os.path.split(fullpath)

    if filename in os.listdir(folder) and not overwrite:
        filename = prefixfilename + "_new" + ".cor"

    with open(os.path.join(folder, filename), "w") as outputfile:

        if not os.access(folder, os.W_OK):
            print('Can not write in the folder: %s'%folder)
            print('File .cor is not written !')
            return None

        firstline_headercolumnsname = "2theta       chi         X           Y           I       "
        format_string = "%.05f   %.05f   %.05f   %.05f   %.02f"
        list_of_data = [twicetheta, chi, data_x, data_y, dataintensity]
        nb_elem_format_string = 5

        if dict_data_spotsproperties is not None:
            print('dict_data_spotsproperties',dict_data_spotsproperties)

            if 'data_spotsproperties' not in dict_data_spotsproperties:
                raise KeyError('"data_spotsproperties" key not found in dict_data_spotsproperties :', dict_data_spotsproperties)

            if verbose>0:
                print('\nIn writefile_cor() :')
                print('total properties in data array of dict_data_spotsproperties',dict_data_spotsproperties['data_spotsproperties'].shape[1])

            ar_list_of_data = np.array(list_of_data).T
            array_data = dict_data_spotsproperties['data_spotsproperties']

            assert len(array_data) == len(twicetheta)

            # print('array_data.shape',array_data.shape)
            ardata =np.hstack((ar_list_of_data,array_data))
            # print('ardata.shape',ardata.shape)

            list_of_data = (ardata.T).tolist()

            
            for _key in dict_data_spotsproperties['columnsname']:
                if _key: # not in ('peak_X', 'peak_Y','peak_Isub'):
                    firstline_headercolumnsname += ' %s'%_key
                    format_string += "   %.03f"
                    nb_elem_format_string+=1
            if verbose>0:
                print('nb of properties to be added',len(list_of_data))
                print('nb_elem_format_string', nb_elem_format_string)

            # prevent  adding other columns from rarely used argument of writefile_cor()
            data_props=None
            data_sat = None

        if data_sat is not None:
            firstline_headercolumnsname += " data_sat"
            format_string += "   %d"
            nb_elem_format_string+=1
            list_of_data += [data_sat]
        if data_props:
            if verbose>0: print('preparing spots props "list_of_data"')
            data_peaks, columnnames = data_props
            for k in range(len(columnnames)):
                firstline_headercolumnsname += " %s" % columnnames[k]
                format_string += "   %.06f"
                nb_elem_format_string+=1
                # TODO clarify ???
                list_of_data += [data_peaks[:, k]]
                #list_of_data += [data_peaks[k, :]]

        firstline_headercolumnsname += "\n"


        if sortedexit and dict_data_spotsproperties is None:
            # to write in decreasing order of intensity (col intensity =4)
            colindex_intensity = 4
            if verbose>0: print("rearranging exp. spots order according to intensity")
            arraydata = np.array(list_of_data).T
            s_ix = np.argsort(arraydata[:, colindex_intensity])[::-1]

            sortedarray = arraydata[s_ix]

            list_of_data = sortedarray.T

        outputfile.write(firstline_headercolumnsname)

        if verbose>0:
            print('nbspots', nbspots)
            print('len(list_of_data)', len(list_of_data))
            print('nb_elem_format_string', nb_elem_format_string)

        ldata = [elem for elem in list_of_data]
        #for elem in ldata:
            #print('len ---', len(elem))
            #print('elem ---', elem)
        if PYTHON3:
            liststrs = [format_string % tuple(list(zip(*ldata))[i]) for i in range(nbspots)]
            #liststrs = [format_string % tuple(list(zip((*list_of_data)))[i]) for i in range(nbspots)]
        else:
            liststrs = [format_string % tuple(zip(*ldata)[i]) for i in range(nbspots)]
        outputfile.write("\n".join(liststrs))

        outputfile.write("\n# File created at %s with IOLaueTools.py" % (time.asctime()))

        if initialfilename:
            outputfile.write("\n# From: %s" % initialfilename)
            outputfile.write("\n# Output Folder: %s" % folder)

        # metadata on calibration detector parameters and nature
        if verbose>0:
            print(' param   in writefile_cor() for prefixfilename %s'%prefixfilename, param)
        if param is not None:
            outputfile.write("\n# Calibration parameters")
            # param is a list
            if isinstance(param, (list, np.ndarray)):
                if len(param) == 6:
                    for par, value in list(zip(CCD_CALIBRATION_PARAMETERS[:6], param)):
                        outputfile.write("\n# %s     :   %s" % (par, value))
                    ypixelsize = param[5] * (1.0 + rectpix)
                    outputfile.write("\n# ypixelsize     :   " + str(ypixelsize))
                elif len(param) == 5:
                    for par, value in list(zip(CCD_CALIBRATION_PARAMETERS[:5], param)):
                        outputfile.write("\n# %s     :   %s" % (par, value))
                else:
                    raise ValueError("5 or 6 calibration parameters are needed!")
            # param is a dict : CCDCalibdict
            elif isinstance(param, (dict,)):
                if verbose>1:
                    print("param is a dict!")
                for key in CCD_CALIBRATION_PARAMETERS:
                    # print('key in CCD_CALIBRATION_PARAMETERS', key)
                    if key in param:
                        # print('key in param', key)
                        outputfile.write("\n# %s     :   %s" % (key, str(param[key])))

        if comments:
            outputfile.write("\n# Comments")
            for line in comments:
                outputfile.write("\n# %s" % line)
    if verbose>0:
        print('Writing .cor file in ',outputfilename)
        print('folder:', folder)
    
    return outputfilename

def get_spotprops_cor(allspotsprops: np.array, filename:str, sortintensities:bool=True,
                                                                defaultminimalnbcolumns:int=5):
    """return other spot properties from .cor file (other than 2theta, Chi, X, Y, Intensity)

    :param allspotsprops: array of all spots properties with shape (nb of spots, nb of props)
    :param filename: str, .cor file to get the columns name
    :param defaultminimalnbcolumns: int, minimal nb of columns (default = 5, 2theta chi X Y I )

    :return: array of spot props (except X Y 2theta chi and I), list of column names
    """
    COLINDEX_INTENSITY_CORFILE = 4  # convention 

    assert len(allspotsprops.shape) == 2
    assert filename.endswith('.cor')

    #print('allspotsprops.shape', allspotsprops.shape)

    # no additional spot properties
    if allspotsprops.shape[1] == defaultminimalnbcolumns:
        return None

    # print('filename', filename, 'allspotsprops  int', allspotsprops[:,4])
    if sortintensities:
        props = allspotsprops[np.argsort(allspotsprops[:, COLINDEX_INTENSITY_CORFILE])[:: -1]]
    else:
        props = allspotsprops

    # list of props
    otherpropsdata = props[:, defaultminimalnbcolumns:].T
    with open(filename, "r") as f:
        columnnames = f.readline().split()[defaultminimalnbcolumns:]
    

    # print('\n\n      get_spotprops_cor()')
    # print('otherpropsdata', otherpropsdata[0])
    # print('columnnames', columnnames)
    # print('\n\n')
    return otherpropsdata, columnnames

def readfile_cor(filename, output_CCDparamsdict=False, output_only5columns=True):
    """
    read peak list in .cor file which is contain 2theta and chi angles for each peak
    .cor file is made at least of 5 columns:  2theta chi pixX pixY I

    :param output_CCDparamsdict: bool, to return or not dict of detector calibration and nature parameters

    :param output_only5columns: bool, to output or not only the 5 first columns (2theta chi pixX pixY I). If False, create a dict dict_spotsproperties = {'columnsname':otherproperties_name, 'data_spotsproperties':otherproperties_data}

    :return: alldata                  #array with all spots properties)
            data_theta, data_chi,
            data_pixX, data_pixY,
            data_I,                            # intensity
            detParam,
            [detector parameters dict (CCDcalib), [dict_spotsproperties



    NOTE: detector parameters has been used previously to compute 2theta and chi (angles of kf)
    from pixX and pixY, ie 2theta chi are detector position independent
    (see find2thetachi for definition of kf)

    #TODO: output 2theta ?
    """
    SKIPROWS = 1
    # read first line
    with open(filename, "r") as f:
        firstline = f.readline()
        unindexeddata = False
        if firstline.startswith("# Unindexed"):
            unindexeddata = True
            SKIPROWS = 7

        import re
        hh = re.sub(' +', ' ', firstline)
        listcolumnsname = hh.split(' ')
        listcolumnsname[-1].strip()

    alldata = np.loadtxt(filename, skiprows=SKIPROWS)

    dict_spotsproperties = {}

    # nbspots, nbcolumns = np.shape(self.alldata)
    sha = np.shape(alldata)
    if len(sha) == 2:
        nbcolumns = sha[1]
        nb_peaks = sha[0]
    elif len(sha) == 1:
        nb_peaks = 1
        nbcolumns = sha[0]

    # having created an empty .cor file may be useless
    assert nb_peaks != 0

    if nb_peaks > 1:

        if nbcolumns == 3:
            data_theta = alldata[:, 0] / 2.0
            data_chi, data_I = alldata.T[1:]
            data_pixX = np.zeros(len(data_chi))
            data_pixY = np.zeros(len(data_chi))
        elif nbcolumns == 5:
            data_theta = alldata[:, 0] / 2.0
            (data_chi, data_pixX, data_pixY, data_I) = alldata.T[1:]
        # case of unindexed file .cor
        elif unindexeddata:
            _, data_I, data_2theta, data_chi, data_pixX, data_pixY = alldata.T
            data_theta = data_2theta / 2.0
        elif nbcolumns > 6:  # .cor file with additional spots properties
            data_2theta, data_chi, data_pixX, data_pixY, data_I = alldata.T[:5]
            data_theta = data_2theta / 2.0

            # listcolumnsname
            firstcolumnindex_properties = 0  # 5
            otherproperties_name = listcolumnsname[firstcolumnindex_properties:]
            otherproperties_name[-1]=otherproperties_name[-1].strip()
            otherproperties_data = alldata[:,firstcolumnindex_properties:]
            dict_spotsproperties = {'columnsname':otherproperties_name, 'data_spotsproperties':otherproperties_data}

    elif nb_peaks == 1:
        if nbcolumns == 3:
            data_theta = alldata[0] / 2.0
            data_chi, data_I = alldata[1:]
            data_pixX = 0
            data_pixY = 0
        elif nbcolumns == 5:
            data_theta = alldata[0] / 2.0
            (data_chi, data_pixX, data_pixY, data_I) = alldata[1:]

        # case of unindexed file .cor
        elif unindexeddata:
            _, data_I, data_2theta, data_chi, data_pixX, data_pixY = alldata
            data_theta = data_2theta / 2.0
        elif nbcolumns > 6:
            data_2theta, data_chi, data_pixX, data_pixY, data_I = alldata[:5]
            data_theta = data_2theta / 2.0
            firstcolumnindex_properties = 0 # 5
            otherproperties_name = listcolumnsname[firstcolumnindex_properties:]
            otherproperties_data = np.array(alldata[firstcolumnindex_properties:])
            dict_spotsproperties = {'columnsname':otherproperties_name, 'data_spotsproperties':otherproperties_data}

    #    print "Reading detector parameters if exist"
    with open(filename, "r") as openf:

        # new way of reading CCD calibration parameters

        CCDcalib = readCalibParametersInFile(openf)
        #print('CCDcalib in readfile_cor() of file %s'%filename, CCDcalib)

        if CCDcalib['dd']:
            detParam = [CCDcalib[key] for key in CCD_CALIBRATION_PARAMETERS[:5]]
            # print("5 CCD Detector parameters read from .cor file: %s"%filename)
        else:
            raise IndexError('Missing explicit values for keys %s in CCDcalib () in file  %s '%(CCD_CALIBRATION_PARAMETERS[:5],filename))
            return
    
    if nb_peaks > 1:
        toreturn = [alldata, data_theta, data_chi,
                    data_pixX, data_pixY, data_I, detParam]
    if nb_peaks == 1:
        toreturn = [np.array([alldata]), np.array([data_theta]), np.array([data_chi]),
                    np.array([data_pixX]), np.array([data_pixY]), np.array([data_I]), detParam]

    
    if output_CCDparamsdict:
        toreturn.append(CCDcalib)

    if not output_only5columns:
        toreturn.append(dict_spotsproperties)
    
    #print('len of output tuple return', len(toreturn))
    return toreturn


def getpixelsize_from_corfile(filename: str):
    """
    return pixel size if written in .cor file


    :param filename: str, full filepath
    """
    xpixelsize = None

    #    print "Reading detector parameters if exist"
    with open(filename, "r") as f:
        find_xpixelsize = False
        find_ypixelsize = False
        find_pixelsize = False

        for line in f:
            if line.startswith("# pixelsize"):
                find_pixelsize = True
                pixelsize = float(line.split(":")[-1])
                break
            if line.startswith("# xpixelsize"):
                find_xpixelsize = True
                xpixelsize = float(line.split(":")[-1])
            elif line.startswith("# ypixelsize"):
                find_ypixelsize = True
                ypixelsize = float(line.split(":")[-1])

    if find_pixelsize:
        return pixelsize

    if find_xpixelsize and find_ypixelsize:
        if xpixelsize != ypixelsize:
            raise ValueError("Pixels are not square!!")

        return xpixelsize
    else:
        return None

def getkfdirection_from_corfile(filename, defautdirection='Z>0'):
    """
    return kf_direction (laue camera/detector position geometry) if written in .cor file
    """

    with open(filename, "r") as f:

        kf_direction =defautdirection
        for line in f:
            if line.startswith("#") and 'kf_direction' in line:
                kf_direction = line.split(":")[-1].strip()
                #print('found kf_direction is:', kf_direction)
                break

    return kf_direction
    
def getdetectordiameter_from_corfile(filename: str, defautdiameter:float=165):
    """
    return kf_direction (laue camera/detector position geometry) if written in .cor file
    """
    assert defautdiameter>0

    with open(filename, "r") as f:

        diam =defautdiameter
        for line in f:
            if line.startswith("#") and 'detectordiameter' in line:
                diam = float(line.split(":")[-1].strip())
                # print('found detectordiameter is:', diam)
                break

    return diam


def readfile_det(filename_det, nbCCDparameters=5, verbose=0):
    """
    read .det file and return calibration parameters and orientation matrix used
    """
    with open(filename_det, 'r') as f:
        i = 0

        mat_line = None
        try:
            for line in f:
                i = i + 1
                if i == 1:
                    calib = np.array(line.split(",")[:nbCCDparameters], dtype=float)
                    if verbose>0:
                        print("calib = ", calib)
                if i == 6:
                    pline = line.replace("[", "").replace("]", "").split(",")
                    mat_line = np.array(pline, dtype=float)
                    if verbose>0:
                        print("UB matrix (calibration step)= \n", mat_line.round(decimals=6))
        finally:
            pass

    return calib, mat_line


def writeCalibFile():
    """
    # TODO:   put here DetectorCalibration.OnSaveCalib()

    """
    pass

def readCalibParametersInFile(openfile, Dict_to_update=None, guessCCDLabel=True):
    """
    read detector parameters and calibration parameter from  .cor or .det file

    If CCDLabel is unknown, it is guessed from pixelsize...

    the parameters are found in the file at line containinf # dd, # xcen , etc...

    .. todo:: we could add dimensions to guess CCDLabel

    return dict of parameters in open file
    """
    #CCD_CALIBRATION_PARAMETERS = ["dd", "xcen", "ycen", "xbet", "xgam",
    #                           "pixelsize", "xpixelsize", "ypixelsize",
    #                           "CCDLabel",  "framedim", "detectordiameter", "kf_direction"]
    List_sharpedParameters = ["# %s" % elem for elem in CCD_CALIBRATION_PARAMETERS]

    # print("List_sharpedParameters", List_sharpedParameters)
    if Dict_to_update is None:
        CCDcalib = {}
    else:
        CCDcalib = Dict_to_update

    for line in openfile:
        if line.startswith(tuple(List_sharpedParameters)):
            key, val = line.split(":")
            key_param = key[2:].strip()
            try:
                val = float(val)
            except ValueError:
                val = readStringOfIterable(val.strip())
            CCDcalib[key_param] = val
        if line.startswith("Material"):
            key, val = line.split(":")
            key_param = key[2:].strip()
            CCDcalib[key_param] = val

    if 'ypixelsize' in CCDcalib:
        CCDcalib['pixelsize'] = CCDcalib['ypixelsize']
    if 'xpixelsize' in CCDcalib:
        CCDcalib['pixelsize'] = CCDcalib['xpixelsize']

    # print('CCDcalib in readCalibParametersInFile of file: %s'%openfile, CCDcalib)

    if 'CCDLabel' not in CCDcalib:  #will recognise from pixelsize...
        CCDcalib['CCDLabel'] = None# DEFAULT_CCDLABEL
        if guessCCDLabel:
            if 'pixelsize' in CCDcalib:
                ps = CCDcalib['pixelsize']
                if abs(ps-0.0795) < 0.004:
                    ccdlabel = 'MARCCD165'
                elif abs(ps-0.073) <= 0.002:
                    ccdlabel = 'sCMOS'
                elif abs(ps-0.0365) <= 0.002:
                    ccdlabel = 'sCMOS_16M'
                elif abs(ps-0.075000001) < 0.0000000001:
                    ccdlabel = 'psl_weiwei'
                elif abs(ps-0.075000002) < 0.0000000001:
                    ccdlabel = 'EIGER_4MCdTe'
                elif abs(ps-0.075000003) < 0.0000000001:
                    ccdlabel = 'EIGER_4MCdTestack'
                elif abs(ps-0.075000004) < 0.0000000001:
                    ccdlabel = 'EIGER_1M'
                elif abs(ps-0.075000005) < 0.0000000001:
                    ccdlabel = 'EIGER_4M'
                elif abs(ps-0.031) <= 0.002:
                    ccdlabel = 'VHR_Feb13'
                elif abs(ps-0.022) <= 0.002:
                    ccdlabel = 'ImageStar_dia_2021'
                elif abs(ps-0.044) <= 0.002:
                    ccdlabel = 'ImageStar_dia_2021_2x2'
                
                elif abs(ps-0.0504) <= 0.002:
                    ccdlabel = 'sCMOS_9M'
                    print('\n\n*******\nWARNING ! This detector could be confused with IMSTAR_bin2\n********\n')
                # elif abs(ps-0.0504) <= 0.001:
                #     ccdlabel = 'IMSTAR_bin2'
                elif abs(ps-0.0252) <= 0.001:
                    ccdlabel = 'IMSTAR_bin1'
                elif abs(ps-0.2) <= 0.001:
                    ccdlabel = 'Alban'
                elif abs(ps-0.4) <= 0.001:
                    ccdlabel = 'RXO'
                elif abs(ps-0.232) <= 0.002:
                    ccdlabel = 'psl_IN_bmp'
                    ccdlabel = 'psl_IN_tif'
                elif abs(ps-0.139) <= 0.002:
                    ccdlabel = 'Alexiane'

                CCDcalib['CCDLabel'] = ccdlabel

    return CCDcalib


def readCalib_det_file(filename_det):
    """
    read .det file and return calibration parameters and orientation matrix used
    """
    with open(filename_det, 'r') as f:
        CCDcalib = readCalibParametersInFile(f)


    calibparam, UB_calib = readfile_det(filename_det, nbCCDparameters=8)

    CCDcalib["framedim"] = calibparam[6:8]
    CCDcalib["detectordiameter"] = max(calibparam[6:8]) * calibparam[5]
    CCDcalib["xpixelsize"] = calibparam[5]
    CCDcalib["pixelsize"] = CCDcalib["xpixelsize"]
    CCDcalib["ypixelsize"] = CCDcalib["xpixelsize"]
    CCDcalib["UB_calib"] = UB_calib

    if "dd" in CCDcalib:
        CCDcalib["CCDCalibParameters"] = [
            CCDcalib[key] for key in CCD_CALIBRATION_PARAMETERS[:5]]
    else:
        CCDcalib["CCDCalibParameters"] = calibparam
        for key, val in list(zip(CCD_CALIBRATION_PARAMETERS[:5], calibparam)):
            CCDcalib[key] = val

    return CCDcalib


def readStringOfIterable(striter):
    """
    extract elements contained in a string and return the list of elements
    (5,9) -> [5,9]
    [2048.0 2048.0] -> [2048,2048]
    """
    if "[" not in striter and not "(" in striter:
        return striter

    ss = striter.strip()[1:-1]

    if "," in ss:
        vals = ss.split(",")
    elif " " in ss:
        vals = ss.split()

    listvals = []
    for elem in vals:
        try:
            val = int(elem)
        except ValueError:
            try:
                val = float(elem)
            except ValueError:
                return striter.strip()
        listvals.append(val)

    return listvals


def writefile_Peaklist(outputprefixfilename, Data_array, overwrite=True,
                                                        initialfilename=None,
                                                        comments=None,
                                                        dirname=None,
                                                        verbose=0):
    """
    Write .dat file

    :param initialfilename: str, image filename
    :param Data_array: numpy array. Shaoe = (nbspots, nb of properties)
    :param dirname: output file dirname (and may be the folder of the image filename)
    :param position_definition:    0 no offset ,1 XMAS offset , 2 fit2D offset
    (see peaksearch)

    :param overwrite: bool, True to overwrite the existing file
                            False to write a file with '_new' added in the name

    :return: full path of created file

    containing data
    one line   of header
    next lines of data
    last lines of comments

    WARNING: compute and add a column 'peak_Itot' in the file

    TODO: should only write things and not compute !! see intensity calculation!
    (peak_I + peak_bkg)

    TODO: to simplify to deal with single peak recording

    TODO: simplify to implement larger number of spot properties
    """
    if verbose>0: print('\n In writefile_Peaklist(): ')
    if Data_array is None:
        if verbose>0:
            print("No data peak to write")
        return
    
    # just one row!
    elif len(Data_array.shape) == 1:
        print("single peak to record!")
        nbpeaks, nbcolumns = 1, Data_array.shape[0]
    else:
        nbpeaks, nbcolumns = Data_array.shape
        #print('Data_array.shape', Data_array.shape)
        if Data_array.shape[0] == 1:
            Data_array = Data_array[0]
    if verbose>0:
        print('Data_array.shape', Data_array.shape)
        print('Found in writefile_Peaklist() Data_array cotresponding to nbpeaks, nbcolumns: ',nbpeaks, nbcolumns)
    # print('Data_array[0]',Data_array[0])

    if dirname is None:
        dirname = os.curdir

    outputfilename = outputprefixfilename + ".dat"

    if outputfilename in os.listdir(os.curdir) and not overwrite:
        outputfilename = outputfilename + "_new" + ".dat"

    if nbpeaks == 1:
        Data_array = np.array([Data_array, Data_array])

    if nbcolumns == 10: # standard 
        (peak_X, peak_Y, peak_I, peak_fwaxmaj, peak_fwaxmin, peak_inclination,
        Xdev, Ydev, peak_bkg, Ipixmax) = Data_array.T

    # elif nbcolumns == 11: # quite obsolete and rare case ?
    #     (peak_X, peak_Y, _, peak_I, peak_fwaxmaj, peak_fwaxmin, peak_inclination,
    #         Xdev, Ydev, peak_bkg, Ipixmax) = Data_array.T
    elif nbcolumns == 12:  # standard + 2 columns for error bars
        (peak_X, peak_Y, peak_I, peak_fwaxmaj, peak_fwaxmin, peak_inclination,
        Xdev, Ydev, peak_bkg, Ipixmax,Xfiterr, Yfiterr) = Data_array.T

    elif nbcolumns == 13:  # standard + 2 columns for error bars
        (peak_X, peak_Y, _, peak_I, peak_fwaxmaj, peak_fwaxmin, peak_inclination,
        Xdev, Ydev, peak_bkg, Ipixmax,Xfiterr, Yfiterr) = Data_array.T

      
    elif nbcolumns == 3: # basic X, Y , I   # from DetectorCalibration board
        # need to set fake data
        (peak_X, peak_Y, peak_I) = Data_array.T
        (peak_fwaxmaj, peak_fwaxmin, peak_inclination,
        Xdev, Ydev, peak_bkg) = np.zeros((6, nbpeaks))
        Ipixmax = 500*np.ones(nbpeaks)

    with open(os.path.join(dirname, outputfilename), "w") as outputfile:
        headerfile = "peak_X peak_Y peak_Itot peak_Isub peak_fwaxmaj peak_fwaxmin "
        headerfile += "peak_inclination Xdev Ydev peak_bkg Ipixmax"
        if nbcolumns in (12, 13): #computerrorbars
             headerfile += " Xfiterr Yfiterr"
        headerfile += '\n'

        outputfile.write(headerfile)

        if nbcolumns < 12:
            if nbpeaks == 1:
                if verbose>1:
                    print("nbcolumns", nbcolumns)

                outputfile.write(
                    "\n%.02f   %.02f   %.02f   %.02f   %.02f   %.02f    %.03f   %.02f   %.02f   %.02f   %d"
                    % (np.round(peak_X[0], decimals=2),
                        np.round(peak_Y[0], decimals=2),
                        np.round(peak_I[0] + peak_bkg[0], decimals=2),
                        np.round(peak_I[0], decimals=2),
                        np.round(peak_fwaxmaj[0], decimals=2),
                        np.round(peak_fwaxmin[0], decimals=2),
                        np.round(peak_inclination[0], decimals=2),
                        np.round(Xdev[0], decimals=2),
                        np.round(Ydev[0], decimals=2),
                        np.round(peak_bkg[0], decimals=2),
                        int(Ipixmax[0])))

                nbpeaks = 1

            else:

                outputfile.write(
                    "\n".join(
                        ["%.02f   %.02f   %.02f   %.02f   %.02f   %.02f    %.03f   %.02f   %.02f   %.02f   %d"
                            % tuple(list(zip(peak_X.round(decimals=2),
                                        peak_Y.round(decimals=2),
                                        (peak_I + peak_bkg).round(decimals=2),
                                        peak_I.round(decimals=2),
                                        peak_fwaxmaj.round(decimals=2),
                                        peak_fwaxmin.round(decimals=2),
                                        peak_inclination.round(decimals=2),
                                        Xdev.round(decimals=2),
                                        Ydev.round(decimals=2),
                                        peak_bkg.round(decimals=2),
                                        Ipixmax))[i]
                            ) for i in list(range(nbpeaks))]))
                nbpeaks = len(peak_X)

        elif nbcolumns in (12, 13):
            if nbpeaks == 1:
                if verbose>1:
                    print("nbcolumns", nbcolumns)

                outputfile.write(
                    "\n%.02f   %.02f   %.02f   %.02f   %.02f   %.02f    %.03f   %.02f   %.02f   %.02f   %d   %.04f   %.04f"
                    % (np.round(peak_X[0], decimals=2),
                        np.round(peak_Y[0], decimals=2),
                        np.round(peak_I[0] + peak_bkg[0], decimals=2),
                        np.round(peak_I[0], decimals=2),
                        np.round(peak_fwaxmaj[0], decimals=2),
                        np.round(peak_fwaxmin[0], decimals=2),
                        np.round(peak_inclination[0], decimals=3),
                        np.round(Xdev[0], decimals=2),
                        np.round(Ydev[0], decimals=2),
                        np.round(peak_bkg[0], decimals=2),
                        int(Ipixmax[0]),
                        np.round(Xfiterr[0],decimals=3),
                        np.round(Yfiterr[0],decimals=3)))

                nbpeaks = 1

            else:
                print('writing XfitErr and YfitErr !')
                outputfile.write(
                    "\n".join(
                        ["%.02f   %.02f   %.02f   %.02f   %.02f   %.02f    %.03f   %.02f   %.02f   %.02f   %d   %.04f   %.04f"
                            % tuple(list(zip(peak_X.round(decimals=2),
                                        peak_Y.round(decimals=2),
                                        (peak_I + peak_bkg).round(decimals=2),
                                        peak_I.round(decimals=2),
                                        peak_fwaxmaj.round(decimals=2),
                                        peak_fwaxmin.round(decimals=2),
                                        peak_inclination.round(decimals=3),
                                        Xdev.round(decimals=2),
                                        Ydev.round(decimals=2),
                                        peak_bkg.round(decimals=2),
                                        Ipixmax,
                                        Xfiterr.round(decimals=3),
                                        Yfiterr.round(decimals=3)))[i]
                            ) for i in list(range(nbpeaks))]))
                nbpeaks = len(peak_X)

        outputfile.write("\n# File created at %s with IOLaueTools.py" % (time.asctime()))
        if initialfilename:
            outputfile.write("\n# From: %s" % initialfilename)
            outputfile.write("\n# Output Folder: %s" % dirname)


        outputfile.write("\n# Comments: nb of peaks %d" % nbpeaks)
        if comments:
            outputfile.write("\n# " + comments)

    if verbose>0:
        print("table of %d peak(s) with %d columns has been written in \n%s"
            % (nbpeaks, nbcolumns, os.path.join(os.path.abspath(dirname), outputfilename)))

    return os.path.join(os.path.abspath(dirname), outputfilename)


def addPeaks_in_Peaklist(
    filename_in, data_new_peaks, filename_out=None, dirname_in=None, dirname_out=None):
    """
    create or update peak list according to a new peaks data
    """

    data_current_peaks = read_Peaklist(filename_in, dirname=dirname_in)

    if data_new_peaks.shape[1] != data_current_peaks.shape[1]:
        raise ValueError("Data to be merged have not the same number of columns")

    # merge data
    raw_merged_data = np.concatenate((data_new_peaks, data_current_peaks), axis=0)

    # sort by peak amplitude (column #3)
    merged_data = raw_merged_data[np.argsort(raw_merged_data[:, 3])[::-1]]

    print("merged_data", merged_data)

    if dirname_in is not None:
        filename_in = os.path.join(dirname_in, filename_in)

    with open(filename_in, "r") as f:
        comments = ""
        incomments = False
        while True:
            line = f.readline()

            if line.startswith("#"):
                incomments = True
                #print(line)
                comments += line

            elif incomments:
                break

    #print(merged_data.shape)

    if filename_out is None:
        filename_out == filename_in
    else:
        if dirname_out is not None:
            filename_out = os.path.join(dirname_out, filename_out)

    writefile_Peaklist(filename_out,
                        merged_data,  # last column is computed inside functions
                        overwrite=1,
                        initialfilename=None,
                        comments=comments,
                        dirname=dirname_out)

    return merged_data

def getcolumnsname_dat(fullpath):
    """
    get the list of columns name of a .dat file (peal list) from peaksearch 

    Parameters
    ----------
    fullpath
        str, full path to .dat file

    Returns
    -------
        list of (str) columns name
    """
    with io.open(fullpath, 'r', encoding='utf-8') as ff:
        _line = ff.readline()
        columnsname = _line.split()

    return columnsname

def getspotsproperties_dat(fullpath, verbose=0):
    """
    return dict of spots properties from .dar file

    Parameters
    ----------
    fullpath
        str, full path to .dat file

    Returns
    -------
        dict of spots properties:  dict_spotsproperties ={'columnsname':list of columns name, 'data_spotsproperties': array, data_spotsproperties  shape = (nb of spots, nb of properties)}
        
    """
    alldata, nbpeaks = readfile_dat(fullpath, returnnbpeaks=True)
    listcolumnsname = getcolumnsname_dat(fullpath)
    nbcolumns = len(listcolumnsname)

    if verbose:
        print('\n   In getspotsproperties_dat(): filename :', fullpath)
        print('nbpeaks', nbpeaks)
        print('alldata.shape', alldata.shape)
        print('raw nbcolumns', nbcolumns)

    
    listcolspotproperties = np.arange(nbcolumns)

    dict_spotsproperties ={}
    
    if nbpeaks > 1:
        nbcolumns = alldata.shape[1]
        data_xyI = np.take(alldata, (0, 1, 3), axis=1)
        data_spotsproperties = np.take(alldata, tuple(listcolspotproperties), axis=1)
    elif nbpeaks == 1:
        data_xyI = np.take(alldata, (0, 1, 3), axis=0)
        data_spotsproperties = np.take(alldata, tuple(listcolspotproperties), axis=0)

    dict_spotsproperties['data_spotsproperties'] = alldata
    dict_spotsproperties['columnsname'] = listcolumnsname

    return dict_spotsproperties


def readfile_dat(filename_in, dirname=None, returnnbpeaks = False):
    """ call simply read_Peaklist()"""
    return read_Peaklist(filename_in, dirname=dirname, returnnbpeaks=returnnbpeaks)


def read_Peaklist(filename_in:str, dirname:str=None, output_columnsname=False,
                  returnnbpeaks:bool=False, maxnumberlines:int=10000, verbose=0):
    """
    read peak list .dat file and return the entire array of spots data
    :param filename_in: str, full path of .dat file if dirname is None, otherwise only filename without folder
    :param maxnumberlines: int, max number of spots (to avoid infinite loop)
    :param output_columnsname: bool, add or not list of column names in the output
    :param returnnbpeaks: bool, add or not nb of spots in the output

    (peak_X,peak_Y,peak_Itot, peak_Isub,peak_fwaxmaj,peak_fwaxmin,
    peak_inclination,Xdev,Ydev,peak_bkg, Ipixmax)
    """
    if verbose: print('\n\nIn read_Peaklist(): ')

    if dirname is not None:
        filename_in = os.path.join(dirname, filename_in)

    SKIPROWS = 1

    with io.open(filename_in, 'r', encoding='utf-8') as ff:
        lineindex = 0
        commentfound = False
        while not commentfound:
            _line = ff.readline()
            if not _line:
                nbdatarows = lineindex-1
                break
            # print('_line',_line)
            # print('line index', lineindex)

            if lineindex > maxnumberlines:  break

            if lineindex == 0:
                columnsname = _line.split()

            if _line.startswith(('# File created','# files created', '# file created)')):
                #print('got comments and nb of spots!')
                nbdatarows = lineindex-1
                commentfound = True
            elif _line.startswith('# Comments: nb of peaks'):
                nbdatarows = int(_line.split('peaks')[-1])
            elif _line.startswith('# Remove_BlackListedPeaks_fromfile'):
                commentfound = True

            lineindex += 1

    if nbdatarows <= 2:  # to fit 1.23 numpy version
        data_peak = np.loadtxt(filename_in, skiprows=SKIPROWS)
    else:
        data_peak = np.loadtxt(filename_in, skiprows=SKIPROWS, max_rows=nbdatarows)

    if len(data_peak.shape) == 1:
        foundNpeaks = 1
    else:
        foundNpeaks = len(data_peak)

    if output_columnsname:
        return data_peak, columnsname

    elif returnnbpeaks:
        return data_peak, foundNpeaks
    else:
        return data_peak


def writefitfile(outputfilename:str, datatooutput, nb_of_indexedSpots:int,
                                            dict_matrices=None,
                                            meanresidues:float=None,
                                            PeakListFilename=None,
                                            columnsname:str=None,
                                            modulecaller:str=None,
                                            refinementtype:str="Strain and Orientation"):
    """
    write a .fit file
    
    :param outputfilename: full path of outputfilename
    """
    # HEADER
    header = "%s Refinement from experimental file: %s\n" % (refinementtype, PeakListFilename)
    modulecallerstr = ""
    if modulecaller is not None:
        modulecallerstr = " with %s" % modulecaller
    header += "File created at %s%s\n" % (time.asctime(), modulecallerstr)
    header += "Number of indexed spots: %d\n" % nb_of_indexedSpots

    print('header', header)

    if "Element" in dict_matrices:
        header += "Element\n"
        header += "%s\n" % str(dict_matrices["Element"])

    if "grainIndex" in dict_matrices:
        header += "grainIndex\n"
        header += "G_%d\n" % dict_matrices["grainIndex"]

    if meanresidues is not None:
        header += "Mean Deviation(pixel): %.3f\n" % meanresidues

    if columnsname:
        header += columnsname.rstrip()

    else:
        header += "spot_index : !!columns name missing !!"

    # FOOTER
    footer = ""

    if "UBmat" in dict_matrices:
        footer += "UB matrix in q= (UB) B0 G* \n"
        #            outputfile.write(str(self.UBB0mat) + '\n')
        footer += str(dict_matrices["UBmat"].round(decimals=9)) + "\n"

    # added O Robach fields   ------------------------
    if "Umat2" in dict_matrices:
        footer += "Umatrix in q_lab= (Umatrix) (B) B0 G* \n"
        #            outputfile.write(str(self.UBB0mat) + '\n')
        footer += str(dict_matrices["Umat2"].round(decimals=9)) + "\n"

    if "Bmat_tri" in dict_matrices:
        footer += "Bmatrix in q_lab= (U) (Bmatrix) B0 G* \n"
        #            outputfile.write(str(self.UBB0mat) + '\n')
        footer += str(dict_matrices["Bmat_tri"].round(decimals=9)) + "\n"

        footer += "(B-I)*1000 \n"
        #            outputfile.write(str(self.UBB0mat) + '\n')
        smattri = (dict_matrices["Bmat_tri"] - np.eye(3)) * 1000.0
        footer += str(smattri.round(decimals=3)) + "\n"

    if ("HKLxyz_names" in dict_matrices) and ("HKLxyz" in dict_matrices):
        footer += "HKL coord. of lab and sample frame axes :\n"
        for k in list(range(6)):
            footer += dict_matrices["HKLxyz_names"][k] + "\t"
            footer += str(dict_matrices["HKLxyz"][k].round(decimals=3)) + "\n"

    # ---------- end O Robach fields
    if "B0" in dict_matrices:
        footer += "B0 matrix in q= UB (B0) G*\n"
        footer += str(dict_matrices["B0"].round(decimals=8)) + "\n"

    if "UBB0" in dict_matrices:
        footer += "UBB0 matrix in q= (UB B0) G* i.e. recip. basis vectors are columns "
        footer += "in LT frame: astar = UBB0[:,0], bstar = UBB0[:,1], cstar = UBB0[:,2]. (abcstar as columns on xyzlab1, "
        footer += "xlab1 = ui, ui = unit vector along incident beam)\n"
        footer += str(dict_matrices["UBB0"].round(decimals=8)) + "\n"

    if "euler_angles" in dict_matrices:
        footer += "Euler angles phi theta psi (deg)\n"
        footer += str(dict_matrices["euler_angles"]) + "\n"

    if "mastarlab" in dict_matrices:
        footer += "matstarlab , abcstar on xyzlab2, ylab2 = ui : astar_lab2 = matstarlab[0:3] "
        footer += ",bstar_lab2 = matstarlab[3:6], cstar_lab2 = matstarlab[6:9] \n"
        footer += str(dict_matrices["matstarlab"].round(decimals=7)) + "\n"

    if "matstarsample" in dict_matrices:
        footer += "matstarsample , abcstar on xyzsample2, xyzsample2 obtained by rotating xyzlab2 "
        footer += "by MG.PAR.omega_sample_frame around xlab2, astar_sample2 = matstarsample[0:3] "
        footer += ",bstar_sample2 = matstarsample[3:6], cstar_lab2 = matstarsample[6:9] \n"
        footer += str(dict_matrices["matstarsample"].round(decimals=8)) + "\n"

    if "devstrain_crystal" in dict_matrices:
        footer += "deviatoric strain in direct crystal frame (10-3 unit)\n"
        footer += str((dict_matrices["devstrain_crystal"] * 1000.0).round(decimals=2)) + "\n"

    if "devstrain_sample" in dict_matrices:
        footer += "deviatoric strain in sample2 frame (10-3 unit)\n"
        footer += str((dict_matrices["devstrain_sample"] * 1000.0).round(decimals=2)) + "\n"

    if "LatticeParameters" in dict_matrices:
        footer += "new lattice parameters\n"
        footer += str(dict_matrices["LatticeParameters"].round(decimals=7)) + "\n"

    if "CCDLabel" in dict_matrices:
        footer += "CCDLabel\n"
        footer += str(dict_matrices["CCDLabel"]) + "\n"

    if "detectorparameters" in dict_matrices:
        footer += "DetectorParameters\n"
        footer += str(dict_matrices["detectorparameters"]) + "\n"

    if "pixelsize" in dict_matrices:
        footer += "pixelsize\n"
        footer += str(dict_matrices["pixelsize"]) + "\n"

    if "framedim" in dict_matrices:
        footer += "Frame dimensions\n"
        footer += str(dict_matrices["framedim"]) + "\n"

    if "Ts" in dict_matrices:
        if dict_matrices["Ts"] is not None:
            footer += "Refined T transform elements in %s\n" % dict_matrices["Ts"][1]
            footer += str(dict_matrices["Ts"][2]) + "\n"

    with open(outputfilename, 'wb') as outputfile:
        np.savetxt(outputfile, datatooutput, fmt="%.6f",
               header=header, footer=footer, comments="#")


def ReadASCIIfile(_filename_data, col_2theta=0, col_chi=1, col_Int=-1, nblineskip=1):
    """ from a file
    return 3 arrays of columns located at index given by
    col_2theta=0, col_chi=1, col_Int=-1:
    [0] theta
    [1] chi
    [2] intensity

    # Quite basic and useless function
    """
    _tempdata = np.loadtxt(_filename_data, skiprows=nblineskip)
    # _tempdata = scipy.io.array_import.read_array(_filename_data, lines = (nblineskip,-1))

    _data_theta = _tempdata[nblineskip - 1 :, col_2theta] / 2.0
    _data_chi = _tempdata[nblineskip - 1 :, col_chi]

    try:
        _data_I = _tempdata[nblineskip - 1 :, col_Int]
    except IndexError:
        print("there are not 5 columns in data.cor file")
        print("I create then a uniform intensity data!")
        _data_I = np.ones(len(_data_theta))
    if (np.array(_data_I) < 0.0).any():
        print("Strange ! I don't like negative intensity...")
        print("I create then a uniform intensity data!")
        _data_I = np.ones(len(_data_theta))

    return (_data_theta, _data_chi, _data_I)


def readfile_fit(fitfilename, verbose=0, readmore=False,
                                        fileextensionmarker=(".fit", ".cor", ".dat"),
                                        returnUnindexedSpots=False,
                                        return_columnheaders=False,
                                        return_toreindex=False):
    """ call alias function readfitfile_multigrains()"""
    return readfitfile_multigrains(fitfilename, verbose, readmore,
                                        fileextensionmarker,
                                        returnUnindexedSpots,
                                        return_columnheaders,
                                        return_toreindex)

def readfitfile_multigrains(fitfilename, verbose=0, readmore=False,
                                        fileextensionmarker=(".fit", ".cor", ".dat"),
                                        returnUnindexedSpots=False,
                                        return_columnheaders=False,
                                        return_toreindex=False):
    """
    JSM version of multigrain.readlt_fit_mg()
    read a single .fit file containing data for several grains

    fileextensionmarker :  '.fit' extension at the end of the line
                            stating that a new grain data starts

    return            : list_indexedgrains_indices,
                        list_nb_indexed_peaks,
                        list_starting_rows_in_data,
                        all_UBmats_flat,
                        allgrains_spotsdata,
                       calibJSM[:, :5],
                       pixdev

                       and in addition if readmore is True:
                       strain6, euler, all_UBB0mats_flat,

               where   list_indexedgrains_indices   : list of indices of indexed grains
                       list_nb_indexed_peaks        : list of numbers of indexed peaks for each grain
                       list_starting_rows_in_data    : list of starting rows in spotsdata for reading grain's spots data

                        all_UBmats_flat           : all 1D 9 elements UBmat matrix
                                                in q = UBmat B0 G* in Lauetools Frame (ki//x)
                                                WARNING! not OR or labframe (ki//y) !!!
                        all_UBB0mats_flat       : all 1D 9 elements UBB0 matrix
                        allgrains_spotsdata        :   array of all spots sorted by grains
                        calibJSM[:, :5]            : contains 5 detector geometric parameters
                        pixdev                    : list of pixel deviations after fit for each grain
                        strain6                    : list of 6 elements (voigt notation) of deviatoric strain
                                                    in 10-3 unit for each grain in CRYSTAL Frame
                                                    from Lauetools calculation
                        euler                    : list of 3 Euler Angles for each grain

    """
    if verbose > 0:
        print("\nIn readfitfile_multigrains(): ")
        print("reading fit file: %s " % fitfilename)

    columns_headers = []

    with open(fitfilename, 'r') as f:

        # search for each start of grain data
        nbgrains = 0
        linepos_grain_list = []
        lineindex = 1
        WrongExtension = False

        for line in f:
            _line = line.rstrip(string.whitespace)
            #if not _line.startswith("# Unindexed and unrefined"):
            #    if _line.endswith(fileextensionmarker):
            #        nbgrains += 1
            #        linepos_grain_list.append(lineindex)
            #    else:
            #        WrongExtension = True
            if _line.startswith(("# Number of indexed spots", "#Number of indexed spots")):
                if verbose > 1: print('got a grain!')
                linepos_grain_list.append(lineindex)
                nbgrains += 1
                
            lineindex += 1
    
    linepos_grain_list.append(lineindex)

    # try:
    #     for line in f:
    #         _line = line.rstrip(string.whitespace)
    #         if not _line.startswith("# Unindexed and unrefined"):
    #             if _line.endswith(fileextensionmarker):
    #                 nbgrains += 1
    #                 linepos_grain_list.append(lineindex)
    #             else:
    #                 WrongExtension = True
    #         lineindex += 1
    #     if WrongExtension:
    #         print('Warning !! Strange extension file for the first line of %s'%fitfilename)
    # finally:
    #     linepos_grain_list.append(lineindex)
    #     f.close()

    if verbose > 0:
        print("nbgrains = ", nbgrains)
        print("linepos_grain_list = ", linepos_grain_list)

    # nothing has been indexed
    if nbgrains == 0:
        return None

    list_indexedgrains_indices = list(range(nbgrains))

    all_UBmats_flat = np.zeros((nbgrains, 9), float)
    all_UBB0mats_flat = np.zeros((nbgrains, 9), float)
    strain6 = np.zeros((nbgrains, 6), float)
    calib = np.zeros((nbgrains, 5), float)
    calibJSM = np.zeros((nbgrains, 7), float)
    euler = np.zeros((nbgrains, 3), float)
    list_nb_indexed_peaks = np.zeros(nbgrains, int)
    list_starting_rows_in_data = np.zeros(nbgrains, int)
    pixdev = np.zeros(nbgrains, float)

    Material_list = []
    GrainName_list = []
    PixDev_list = []

    dataspots_Unindexed = []

    # read .fit file for each grain

    UBmat = np.zeros((3, 3), dtype=np.float32)
    UBB0mat = np.zeros((3, 3), dtype=np.float32)
    strain = np.zeros((3, 3), dtype=np.float32)
    strain2 = np.zeros((3, 3), dtype=np.float32)

    matrixfound = 0
    calibfound = 0
    calibfoundJSM = 0
    pixdevfound = 0
    strainfound = 0
    strainfound2 = 0
    eulerfound = 0
    linecalib = 0
    linepixdev = 0
    #linestrain = 0
    lineeuler = 0

    with open(fitfilename, 'r') as f:

        unindexedspots = False
        for grain_index in list(range(nbgrains)):

            iline = linepos_grain_list[grain_index]

            nb_indexed_spots = 0
            ubb0found = 0
            while iline < linepos_grain_list[grain_index + 1]:

                line = f.readline()
                #             print "iline =%d line" % iline, line
                if line.startswith(("# Number of indexed spots", "#Number of indexed spots")):
                    if verbose > 1: print("iline =%d line" % iline, line)
                    try:
                        nb_indexed_spots = int(line.split(":")[-1])
                    except ValueError:
                        if verbose > 1: print("number of indexed spots should placed after ':' ")

                elif line.startswith(("# Number of unindexed spots", "#Number of unindexed spots")):
                    nb_indexed_spots = 0
                    nb_UNindexed_spots = int(line.split(":")[-1])
                    unindexedspots = True

                elif line.startswith(("# Mean Pixel Deviation", "#Mean Deviation", "#Mean Pixel Deviation")):
                    meanpixdev = float(line.split(":")[-1])
                    PixDev_list.append(meanpixdev)

                elif line.startswith("#Element"):
                    line = f.readline()
                    Material_list.append(line.rstrip("\n"))
                    iline += 1
                elif line.startswith("#grainIndex"):
                    line = f.readline()
                    GrainName_list.append(line.rstrip("\n"))

                    iline += 1
                elif line.startswith(("spot#", "#spot", "##spot", "# Spot")):
                    if not unindexedspots:
                        columns_headers = line.replace("#", "").split()

                    if nb_indexed_spots > 0:
                        nbspots = nb_indexed_spots

                        dataspots = []
                        for _ in list(range(nbspots)):
                            line = f.readline()
                            iline += 1
                            dataspots.append(
                                line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split())

                        dataspots = np.array(dataspots, dtype=np.float32)

                    elif nb_UNindexed_spots > 0:
                        nbspots = nb_UNindexed_spots

                        dataspots_Unindexed = []
                        for _ in list(range(nbspots)):
                            line = f.readline()
                            iline += 1
                            dataspots_Unindexed.append(
                                line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split())

                        dataspots_Unindexed = np.array(dataspots_Unindexed, dtype=np.float32)

                elif line.startswith("#UB matrix"):
                    matrixfound = 1

                    #lineendspot = iline - 1
                    # print "matrix found"
                elif line.startswith("#UBB0 matrix"):
                    ubb0found = 1
                elif line.startswith("#Sample"):
                    calibfound = 1
                    linecalib = iline + 1
                elif line.startswith(("# Calibration", "#Calibration")):
                    calibfoundJSM = 1
                    linecalib = iline + 1
                elif line.startswith("#pixdev"):
                    pixdevfound = 1
                    linepixdev = iline + 1
                elif line.startswith("#deviatoric strain in direct crystal"):
                    strainfound = 1
                elif line.startswith("#deviatoric strain in sample2"):
                    strainfound2 = 1
                elif line.startswith("#Euler"):
                    eulerfound = 1
                    lineeuler = iline + 1

                if verbose > 1: print('Eulers Angles found', eulerfound)
                
                if matrixfound:
                    for jline_matrix in list(range(3)):
                        line = f.readline()
                        lineval = line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split()
                        UBmat[jline_matrix, :] = np.array(lineval, dtype=float)
                        iline += 1
                    #                 print "got UB matrix:", UBmat
                    matrixfound = 0

                if ubb0found:
                    jline_matrix = 0
                    while(jline_matrix<3):
                        line = f.readline()
                        if line.startswith(("in LT frame","xlab1 = ui")): # there could be additionnal text before data
                            iline += 1
                            continue
                        lineval = (line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split())
                        UBB0mat[jline_matrix, :] = np.array(lineval, dtype=float)
                        jline_matrix += 1

                    ubb0found = 0

                if strainfound:
                    for jline_matrix in list(range(3)):
                        line = f.readline()
                        lineval = (line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split())
                        strain[jline_matrix, :] = np.array(lineval, dtype=float)
                        iline += 1
                    #                 print "got strain matrix:", strain
                    strainfound = 0

                # actually dev strain in sample2 frame is not considered there but it will be recomputed later ...
                if strainfound2:
                    for jline_matrix in list(range(3)):
                        line = f.readline()
                        lineval = (line.rstrip("\n").replace("#", "").replace("[", "").replace("]", "").split())
                        strain2[jline_matrix, :] = np.array(lineval, dtype=float)
                        iline += 1
                    #                 print "got strain matrix:", strain
                    strainfound2 = 0

                if calibfoundJSM:
                    calibparam = []
                    for _ in list(range(7)):
                        line = f.readline()
                        val = float(line.split(":")[-1])
                        calibparam.append(val)
                        iline += 1
                    #                 print "got calibration parameters:", calibparam
                    calibJSM[grain_index, :] = calibparam
                    calibfoundJSM = 0

                if calibfound & (iline == linecalib):
                    calib[grain_index, :] = np.array(line.split(",")[:5], dtype=float)
                    # print "calib = ", calib[grain_index,:]
                if eulerfound & (iline == lineeuler):
                    euler[grain_index, :] = np.array(
                        line.replace("[", "").replace("#", "").replace("]", "").split()[:3], dtype=float)
                    # print "euler = ", euler[grain_index,:]
                if pixdevfound & (iline == linepixdev):
                    pixdev[grain_index] = float(line.rstrip("\n"))
                    # print "pixdev = ", pixdev[grain_index]

                iline += 1

            list_nb_indexed_peaks[grain_index] = np.shape(dataspots)[0]

            #        if min_matLT == True :
            #            matmin, transfmat = FindO.find_lowest_Euler_Angles_matrix(UBmat)
            #            UBmat = matmin
            #            print "transfmat \n", list(transfmat)
            #            # transformer aussi les HKL pour qu'ils soient coherents avec matmin
            #            hkl = data_fit[:, 2:5]
            #            data_fit[:, 2:5] = np.dot(transfmat, hkl.transpose()).transpose()
            all_UBmats_flat[grain_index, :] = np.ravel(UBmat)
            all_UBB0mats_flat[grain_index, :] = np.ravel(UBB0mat)

            # xx yy zz yz xz xy
            # voigt notation
            strain6[grain_index, :] = np.array([strain[0, 0],
                                                strain[1, 1],
                                                strain[2, 2],
                                                strain[1, 2],
                                                strain[0, 2],
                                                strain[0, 1]])

            if grain_index == 0:
                allgrains_spotsdata = dataspots * 1.0
            elif grain_index:
                allgrains_spotsdata = np.row_stack((allgrains_spotsdata, dataspots))

    for grain_index in list(range(1, nbgrains)):
        list_starting_rows_in_data[grain_index] = (list_starting_rows_in_data[grain_index - 1]
                                                    + list_nb_indexed_peaks[grain_index - 1])

    pixdev = np.array(PixDev_list, dtype=np.float32)

    if verbose > 0:
        print("list_indexedgrains_indices = ", list_indexedgrains_indices)
        print("all_UBmats_flat = ")
        print(all_UBmats_flat)
        print("list_nb_indexed_peaks = ", list_nb_indexed_peaks)
        print("list_starting_rows_in_data = ", list_starting_rows_in_data)
        print("pixdev = ", pixdev.round(decimals=4))
        print("strain6 = \n", strain6.round(decimals=2))
        print("euler = \n", euler.round(decimals=3))

    if not readmore:
        toreturn = (list_indexedgrains_indices,
                    list_nb_indexed_peaks,
                    list_starting_rows_in_data,
                    all_UBmats_flat,
                    allgrains_spotsdata,
                    calibJSM[:, :5],
                    pixdev)
    elif readmore:  # actually dev strain in sample2 frame is not saved there but it will be recomputed later ...
        toreturn = (list_indexedgrains_indices,
                    list_nb_indexed_peaks,
                    list_starting_rows_in_data,
                    all_UBmats_flat,
                    allgrains_spotsdata,
                    calibJSM[:, :5],
                    pixdev,
                    strain6,
                    euler,
                    all_UBB0mats_flat)
    if return_toreindex:
        toreturn = (list_indexedgrains_indices,
                    list_nb_indexed_peaks,
                    pixdev,
                    Material_list,
                    all_UBmats_flat,
                    calibJSM[:, :5])


    if columns_headers is not []:
        dict_column_header = {}
        for k, col_name in enumerate(columns_headers):
            dict_column_header[col_name] = k
    else:
        if return_columnheaders:
            raise ValueError("problem reading columns name")

    if returnUnindexedSpots:
        _res = toreturn, dataspots_Unindexed
    else:
        _res = toreturn

    if return_columnheaders:
        return _res, dict_column_header
    else:
        return _res

def readfitfile_comments(fitfilepath):
    """read comments in .fit file and return corresponding strings
    #CCDLabel
    #pixelsize
    #Frame dimensions
    #DetectorParameters
    """
    dictcomments = {}
    ccdlabelflag = False
    pixelsizeflag = False
    framedimflag = False
    detectorflag = False

    with open(fitfilepath, 'r') as f:
        nblines = 0
        for line in f.readlines():
            nblines += 1
        f.seek(0)
        lineindex = 0
        while lineindex < nblines:
            line = f.readline()
            # print('lineeeeeeeeee', line)
            if ccdlabelflag:
                dictcomments['CCDLabel'] = line.split('#')[1].strip()
                ccdlabelflag = False
            if pixelsizeflag:
                dictcomments['pixelsize'] = line.split('#')[1].strip()
                pixelsizeflag = False
            if framedimflag:
                dictcomments['framedim'] = line.split('#')[1].strip()
                framedimflag = False
            if detectorflag:
                dictcomments['detectorparameters'] = line.split('#')[1].strip()
                detectorflag = False

            if line.startswith(('#CCDLabel', "# CCDLabel")):
                ccdlabelflag = True
            if line.startswith(('#pixelsize', "# pixelsize")):
                pixelsizeflag = True
            if line.startswith(('#Frame dimensions', "# Frame dimensions")):
                framedimflag = True
            if line.startswith(('#DetectorParameters', "# DetectorParameters")):
                detectorflag = True
            lineindex += 1

    return dictcomments


def convert_fit_to_cor(fitfilepath, verbose=0):
    """ convert .fit file to .cor file"""

    col_2theta, col_chi = 9, 10
    col_Xexp, col_Yexp = 7, 8
    col_intensity = 1

    # output_corfilepath = fitfilepath[-4:] + '.cor'

    folder, filename = os.path.split(fitfilepath)
    prefixfilename = filename.rsplit(".", 1)[0]

    if verbose:
        print('\n in convert_fit_to_cor')
        print('filename', filename)
        print('prefixfilename: %s\n'%prefixfilename)

    # read .fit file
    res = readfitfile_multigrains(fitfilepath)

    alldata = res[4]

    #   (nb spots,  nb properties/spots)  sorted by grainindex
    if verbose:
        print('alldata.shape', alldata.shape)

    (twicetheta, chi,
    data_x, data_y, dataintensity) = (alldata[:, col_2theta], alldata[:, col_chi],
                            alldata[:, col_Xexp], alldata[:, col_Yexp], alldata[:, col_intensity])

    dictcoms = readfitfile_comments(fitfilepath)
    if 'pixelsize' in dictcoms:
        pixelsize = dictcoms['pixelsize']
        # print('pix', pixelsize)
    if 'CCDLabel' in dictcoms:
        CCDLabel = dictcoms['CCDLabel']
        # print('ccdlabel',CCDLabel)

    if 'detectorparameters' in dictcoms:
        detparams = dictcoms['detectorparameters']
        detectorparameters = readStringOfIterable(detparams)
        # print('detparam',detectorparameters)

    listfield = ["dd", "xcen", "ycen", "xbet", "xgam", "pixelsize",
                                "xpixelsize", "ypixelsize", "CCDLabel"]
    listval = detectorparameters + [pixelsize, pixelsize, pixelsize, CCDLabel]
    dictparam = {}
    for key, val in zip(listfield, listval):
        dictparam[key] = val

    # print('dictparam in convert_fit_to_cor()', dictparam)

    # write .cor file
    filecor = writefile_cor(prefixfilename, twicetheta, chi, data_x, data_y, dataintensity,
                                    param=dictparam,
                                    dirname_output=folder,
                                    overwrite=1)

    return os.path.join(folder, filecor)


def read3linesasMatrix(fileobject):
    """
    return matrix from reading 3 lines in fileobject
    """
    matrix = np.zeros((3, 3))
    iline = 0
    for i in list(range(3)):
        line = fileobject.readline()

        listval = re.split("[ ()\[\)\;\,\]\n\t\a\b\f\r\v]", line)
        listelem = []
        for elem in listval:
            if elem not in ("",):
                val = float(elem)
                listelem.append(val)

        #print("listelem", listelem)

        matrix[i, :] = np.array(listelem, dtype=float)
        iline += 1
    if iline == 3:
        return matrix


def readListofIntegers(fullpathtoFile):
    """ parse a string of list of integers """
    fileobject = open(fullpathtoFile, "r")

    nbElements = 0

    lines = fileobject.readlines()
    listelem = []
    for line in lines:

        listval = re.split("[ ()\[\)\;\:\!\,\]\n\t\a\b\f\r\v]", line)

        for elem in listval:
            if elem not in ("",):
                try:
                    val = int(float(elem))
                except ValueError:
                    return None
                listelem.append(val)
                nbElements += 1

    return listelem

def read_roisfile(fullpathtoFile):
    """ read a file where each line is x,y,boxx,boxy (all integers or will be converted in integers)
    """
    with open(fullpathtoFile, 'r') as f:

        nbRois = 0

        lines = f.readlines()
        listrois = []
        for line in lines:

            listval = re.split("[ ()\[\)\;\:\!\,\]\n\t\a\b\f\r\v]", line)
            nbelems = 0
            listroielems = []
            for elem in listval:
                if elem not in ("",):
                    try:
                        val = int(float(elem))
                    except ValueError:
                        print("can't convert %s into integer...! I give up"%elem)
                        return None
                    listroielems.append(val)
                    nbelems += 1
            if nbelems == 4:
                listrois.append(listroielems)
                nbRois += 1
            else:
                print("\n\n**********\n\nSorry. I can't extract 4 integers from the line %s. "
                    "I give up...****\n\n" % line)
                return None

    return listrois


def readListofMatrices(fullpathtoFile):
    """ Read ASCII file containing n matrices elements. Must be 9*n float numbers

    :param fullpathtoFile: full path
    :type fullpathtoFile: str
    :raises ValueError: if number of floats is not a multiple of 9
    :return: nb of matrices found, list of matrices
    :rtype: (int, list)
    """

    fileobject = open(fullpathtoFile, "r")

    nbElements = 0

    lines = fileobject.readlines()
    listelem = []
    for line in lines:

        listval = re.split("[ ()\[\)\;\,\]\n\t\a\b\f\r\v]", line)

        for elem in listval:
            if elem not in ("",):
                val = float(elem)
                listelem.append(val)
                nbElements += 1

    if (nbElements % 9) != 0:
        raise ValueError("Number of elements is not a multiple of 9")

    nbMatrices = nbElements // 9
    matrices = np.array(listelem, dtype=float).reshape((nbMatrices, 3, 3))
    return nbMatrices, matrices


def readCheckOrientationsFile(fullpathtoFile):
    """
    read .ubs file

    return tuple of two elements:
    [0] nb of Material in output
    [1] list of infos:
        [0] fileindices ROI infos
        [1] Material for indexation
        [2] Energy max and minimum matching rate threshold (nb of coincidence / nb of theo. spots)
        [3] nb of matrices to be tested
        [4] matrix or list of matrices

    # design of .mats file aiming at giving infos of guesses UB matrix solutions
    prior to indexation from scratch

    Hierarchical tree structure  FileIndex  / Grain / Material / EnergyMax / MatchingThreshold / Matrix(ces)

    --- Fileindex1  --Grain- Material 1-1 -EnergyMax -MatchingThreshold- Matrix(ces)
                    |
                    --Grain- Material 1-2 --- Matrix(ces)
                    |
                    --Grain -  ...

    --- Fileindex2  --- Material 2-1 --- Matrix(ces)
                    |
                    --- Material 2-2 --- Matrix(ces)
                    |
                    ---  ...

    --- Fileindex3  --- Material 3-1 --- Matrix(ces)
                    |
                    --- Material 3-2 --- Matrix(ces)
                    |
                    ---  ...

    When using this file, current fileindex will be searched among the  Fileindex3 sets.
    If found, guessed Material and matrices will be then tested before indexation from scratch

    return: list of CheckOrientation

    where each CheckOrientation is a list of:
    - File index (list or -2 for all images)
    - Grain index
    - Material
    - Energy Max
    - MatchingThreshold
    - Matrix(ces)

    .. note:: this .ubs file must be accompanied by .irp with the same nb of materials in the same order (to perform the refinement)

    example.ubs--------
    $FileIndex
    [0,1,2,3,4,5]
    $Grain
    0
    $Material
    Current
    $EnergyMax
    22
    $MatchingThreshold
    50
    $Matrix
    [[0.5165,0.165,-.95165],
    [0.3198951498,-0.148979,0.123126],
    [-.4264896,.654128,-.012595747]]
    $Material
    Cu
    $Matrix
    [[0.8885165,0.0000165,-.777795165],
    [0.100003198951498,-74440.148979,0.155242423126],
    [-.54264896,.99999654128,-.572785747]]
    $FileIndex
    [6,7,8]
    $Material
    Ge
    $Matrix
    [[0.5165,0.165,-.95165],
    [0.3198951498,-0.148979,0.123126],
    [-.4264896,.654128,-.012595747]]
    [[0.8885165,0.0000165,-.777795165],
    [0.100003198951498,-74440.148979,0.155242423126],
    [-.54264896,.99999654128,-.572785747]]
    $Material
    Current
    $Matrix
    [[0.8885165,0.0000165,-.777795165],
    [0.100003198951498,-74440.148979,0.155242423126],
    [-.54264896,.99999654128,-.572785747]]
    $FileIndex
    All
    $Material
    Current
    $Matrix
    [[0.5165,0.165,-.95165],
    [0.3198951498,-0.148979,0.123126],
    [-.4264896,.654128,-.012595747]]
    END

    substrate_and_grains.ubs--------
    $FileIndex
    All
    $Grain
    0
    $Material
    Si
    $EnergyMax
    22
    $MatchingThreshold
    50
    $Matrix
    [[0.5165,0.165,-.95165],
    [0.3198951498,-0.148979,0.123126],
    [-.4264896,.654128,-.012595747]]
    $Grain
    1
    $Material
    Cu
    $Matrix
    [[0.8885165,0.0000165,-.777795165],
    [0.100003198951498,-74440.148979,0.155242423126],
    [-.54264896,.99999654128,-.572785747]]
    END
    """

    List_posImageIndex = []
    List_posGrain = []
    List_posMaterial = []
    List_posEnergyMax = []
    List_posMatchingThreshold = []
    List_posMatrices = []

    List_CheckOrientations = []

    known_values = [False for k in list(range(6))]
    Current_CheckOrientationParameters = [0 for k in list(range(6))]

    with open(fullpathtoFile, 'r') as f:
    
        lineindex = 0
        while 1:
            line = f.readline()
            #print("line file.ubs",line)
            if line.startswith("$"):
                if line.startswith("$FileIndex"):
                    line = str(f.readline())
                    #print("$FileIndex: ", line)
                    list_indices = getfileindex(line)
                    Current_CheckOrientationParameters[0] = list_indices
                    known_values[0] = True
                    List_posImageIndex.append(lineindex)
                    lineindex += 1
                elif line.startswith("$Grain"):
                    line = f.readline()
                    grain_index = int(line)
                    Current_CheckOrientationParameters[1] = grain_index
                    known_values[1] = True
                    List_posGrain.append(lineindex)
                    lineindex += 1
                elif line.startswith("$Material"):
                    line = f.readline()
                    key_material = str(line).strip()
                    Current_CheckOrientationParameters[2] = key_material
                    known_values[2] = True
                    List_posMaterial.append(lineindex)
                    lineindex += 1
                elif line.startswith("$EnergyMax"):
                    line = f.readline()
                    energymax = int(line)
                    Current_CheckOrientationParameters[3] = energymax
                    known_values[3] = True
                    List_posEnergyMax.append(lineindex)
                    lineindex += 1
                elif line.startswith("$MatchingThreshold"):
                    line = f.readline()
                    matchingthreshold = float(line)
                    Current_CheckOrientationParameters[4] = matchingthreshold
                    known_values[4] = True
                    List_posMatchingThreshold.append(lineindex)
                    lineindex += 1
                elif line.startswith("$Matrix"):
                    nbMatrices, matrices, nblines, posfile = readdataasmatrices(f)
                    #print("nbMatrices,matrices, nblines, posfile",
                        # nbMatrices,
                        # matrices,
                        # nblines,
                        # posfile)

                    Current_CheckOrientationParameters[5] = matrices
                    known_values[5] = True
                    List_posMatrices.append(lineindex)

                    #print("Current_CheckOrientationParameters", Current_CheckOrientationParameters)
                    #print("known_values", known_values)

                    List_CheckOrientations.append(copy.copy(Current_CheckOrientationParameters))

                    if posfile != -1:
                        f.seek(posfile)
                        for _ in list(range(nblines)):
                            f.readline()
                            lineindex += 1
                    else:
                        
                        break

    return List_CheckOrientations


def getfileindex(str_expression):
    #print("str_expression", str_expression)
    if str_expression.strip() in ("all", "All"):
        return -1

    list_val = str_expression.strip("[]()\n").split(",")
    #print("list_val", list_val)
    integerlist = [int(float(elem)) for elem in list_val]
    return integerlist


def readdataasmatrices(fileobject):
    """ read matrices in ubs file?
    """
    posfile = fileobject.tell()

    nbElements = 0

    nblines = 1
    lines = []

    while True:
        line = str(fileobject.readline())
        #print("line matrix", line)
        if line.startswith("$"):
            break
        if line.strip() in ("END",):
            posfile = -1
            break
        lines.append(line)
        nblines += 1

    listelem = []
    for line in lines:

        listval = re.split("[ ()\[\)\;\,\]\n\t\a\b\f\r\v]", line)

        for elem in listval:
            if elem not in ("",):
                val = float(elem)
                listelem.append(val)
                nbElements += 1

                #print('listelem', listelem)

    if (nbElements % 9) != 0:
        raise ValueError("Number of elements is not a multiple of 9")

    nbMatrices = nbElements // 9
    matrices = np.array(listelem, dtype=float).reshape((nbMatrices, 3, 3))

    return nbMatrices, matrices, nblines - 1, posfile


def writefile_log(output_logfile_name="lauepattern.log", linestowrite=[[""]]):
    """
    TODO: maybe useless ?
    """
    filou = open(output_logfile_name, "w")
    aecrire = linestowrite
    for line in aecrire:
        lineData = "\t".join(line)
        filou.write(lineData)
        filou.write("\n")
    filou.close()


def Writefile_data_log(grainspot, index_of_grain, linestowrite=[[""]], cst_energykev=CST_ENERGYKEV):
    """
    write a log data file of simulation
    """
    for elem in grainspot:
        linestowrite.append([str(index_of_grain),
                            str(elem.Millers[0]),
                            str(elem.Millers[1]),
                            str(elem.Millers[2]),
                            str(elem.EwaldRadius * cst_energykev),
                            str(elem.Twicetheta),
                            str(elem.Chi),
                            str(elem.Xcam),
                            str(elem.Ycam)])


def writefilegnomon(gnomonx, gnomony, outputfilename, dataselected):
    """
    write file with gnomonic coordinates
    """
    linestowrite = []
    linestowrite.append(["gnomonx gnomony 2theta chi I #spot"])
    nb = len(gnomonx)
    for i in list(range(nb)):
        linestowrite.append([str(gnomonx[i]),
                            str(gnomony[i]),
                            str(2.0 * dataselected[0][i]),
                            str(dataselected[1][i]),
                            str(dataselected[2][i]),
                            str(i)])

    naname = outputfilename + ".gno"
    outputfile = open(naname, "w")
    for line in linestowrite:
        lineData = "\t".join(line)
        outputfile.write(lineData)
        outputfile.write("\n")
    outputfile.close()


def ReadSummaryFile(filename, dirname=None):
    """
    read summary .dat file generated by multigrain

    one line per grain and per image
    """
    fullpath = filename
    if dirname is not None:
        fullpath = os.path.join(dirname, filename)

    f = open(fullpath, "r")
    # skip first line
    f.readline()

    # read columns name
    columns = f.readline()
    list_cols = columns.split(" ")
    list_column_names = []
    dict_column_names = {}
    for k, elem in enumerate(list_cols):
        list_column_names.append(elem)
        dict_column_names[elem] = k

    data = np.loadtxt(f, dtype=np.float32)  # , comments, delimiter, converters, skiprows, usecols, unpack, ndmin)

    f.close()

    # remove last elem = '\n'
    del dict_column_names["\n"]
    return data, list_column_names[:-1], dict_column_names


def createselecteddata(tupledata_theta_chi_I, _listofselectedpts, _indicespotmax):
    """
    select part of peaks in peaks data

    From theta,chi,intensity
    returns the same arrays with less points (if _indicespotmax>=1)
    TODO: to document, and to extend to posX and posY
    """
    _data_theta, _data_chi, _data_I = tupledata_theta_chi_I

    if _indicespotmax < 1:
        # all selected data are considered
        _nbmax = len(_listofselectedpts)
    else:
        _nbmax = min(_indicespotmax, len(_listofselectedpts))

    cutlistofselectedpts = _listofselectedpts[:_nbmax]
    # print cutlistofselectedpts
    # print "nb of selected data points",len(cutlistofselectedpts)
    if cutlistofselectedpts is None:
        _dataselected = np.array(np.zeros((3, len(_data_theta)), dtype=np.float32))
        _dataselected[1] = _data_chi
        _dataselected[0] = _data_theta
        _dataselected[2] = _data_I
    else:
        # _dataselected=np.array(zeros((3,len(cutlistofselectedpts)),'float'))
        _dataselected = np.array(np.zeros((3, len(cutlistofselectedpts)), dtype=np.float32))
        _dataselected[0] = np.take(_data_theta, cutlistofselectedpts)
        _dataselected[1] = np.take(_data_chi, cutlistofselectedpts)
        _dataselected[2] = np.take(_data_I, cutlistofselectedpts)

    return (_dataselected, _nbmax)






# --- -----  read write Parameters file
class readwriteParametersFile:
    """
    class in (old) developement
    """

    def __init__(self):
        pass

    def loadParamsFile(self, filename, dirname=None):
        with open(filename) as fh:
            self.attrs = []
            for line in fh:
                if not line.startswith(("#", "!", "-")):
                    print("line", line)
                    s = line.strip(" \n").split("=")
                    print("s", s)
                    attr_name = s[0].strip()
                    print("attr_names", attr_name)
                    if s != [""]:
                        if len(s) == 2:
                            setattr(self, attr_name, s[1])
                        elif len(s) == 3:
                            setattr(self, attr_name, s[1:])
                        self.attrs.append(attr_name)

    def getParamsDict(self):
        print(self.attrs)


# ---  --- XMAS file related functions
def readxy_XMASind(filename):
    """
    read XMAS indexation file
    and return:
    x(exp)  y(exp)  h  k  l  ang.dev. xdev(pix)  ydev(pix)  energy(keV)  theta(deg)  intensity   integr  xwidth(pix)   ywidth(pix)  tilt(deg)  rfact   pearson  xcentroid(pix)   ycentroid(pix)

    usage:
    dataindXMAS = readxy_XMASind('Ge_run41_1_0003_1.ind')
    X_XMAS = dataindXMAS[:, 0]
    Y_XMAS = dataindXMAS[:, 1]

    # for nspots=array([0,1,2,3,4])
    pixX, pixY = np.transpose(np.take(dataindXMAS[:, :2], (1, 5, 4, 10, 7), axis=0))

    # not much used now!
    """
    f = open(filename, "r")

    _ = f.readline()  #filename_mccd
    f.readline()
    f.readline()
    l = f.readline()
    nb_peaks = int(l.split()[-1])
    f.readline()
    datalines = []
    for _ in list(range(nb_peaks)):
        datalines.append(f.readline().split())

    return np.array(datalines, dtype=float)


def read_cri(filecri):
    """
    file .cri of XMAS
    """
    # fichier type : attention parametres a b c en nanometres

    # Al2O3 crystal (hexagonal axis)
    # 167
    # 0.47588   0.47588   1.29931  90.00000  90.00000 120.00000
    # 2
    # Al001    0.00000   0.00000   0.35230   1.00000
    # O0001    0.30640   0.00000   0.25000   1.00000

    uc = np.zeros(6, dtype=np.float32)
    element_name = []
    #sg_num = 0

    VERBOSE = 0

    print("reading crystal structure from file :  ", filecri)
    f = open(filecri, "r")
    i = 0
    try:
        for line in f:
            if (i == 0) & VERBOSE:
                print("comment : ", line[:-1])
            if i == 1:
                if VERBOSE:
                    print("space group number : ", line[:-1])
                #sg_num = int(line.split()[0])
                # print sg_num
            if i == 2:
                if VERBOSE:
                    print("unit cell parameters (direct space) : ", line[:-1])
                for j in list(range(6)):
                    if j < 3:
                        # print line.split()[j]
                        uc[j] = float(line.split()[j]) * 1.0
                    else:
                        uc[j] = float(line.split()[j])
                # print uc
            if i == 3:
                if VERBOSE:
                    print("number of atoms in asymmetric unit : ", line[:-1])
                num_at = int(line.split()[0])
                # print num_at

            if i > 3:
                # print "new line", line
                # print line.split()
                if np.size(line.split()) > 0:
                    if ((line.split()[0])[1]).isalpha():
                        element_name.append((line.split()[0])[0:2])
                    else:
                        element_name.append((line.split()[0])[0:1])
            i = i + 1

    finally:
        #linetot = i
        f.close()

    linestart = 4
    #lineend = num_at + 4

    # to use loadtxt ?...
    element_coord_and_occ = np.genfromtxt(filecri, usecols=(1, 2, 3, 4), skip_header=linestart)

    if VERBOSE:
        print("element_coord_and_occ = \n", element_coord_and_occ)

        print("element_name =\n", element_name)

        print("%d atom(s) in asymmetric unit :" % num_at)
        if num_at > 1:
            for i in list(range(num_at)):
                print(element_name[i], "\t", element_coord_and_occ[i, :])
        else:
            print(element_name, "\t", element_coord_and_occ)

    return uc


def readfile_str(filename, grain_index):
    """
    read XMAS .str file

    return for one grain (WARNING: grain_index  starting from 1)
    data_str:
    matstr
    calib
    dev_str

    WARNING: endline does not have space character, this is really annoying
    upgraded scipy.io.array_import to np.genfromtxt
    TODO: to be refactored (JSM Feb 2012)
    """

    print("reading info from STR file : \n", filename)
    # print "peak list, calibration, strained orientation matrix, deviations"
    # print "change sign of HKL's"
    f = open(filename, "r")
    i = 0
    grainfound = 0
    calib = np.zeros(5, dtype=np.float32)
    # x(exp)  y(exp)  h  k  l xdev  ydev  energy  dspace  intens   integr
    # xwidth   ywidth  tilt  rfactor   pearson  xcentroid  ycentroid
    # 0 x(exp)    # 1 y(exp)
    # 2 3 4 h  k  l
    # 5 6 xdev  ydev
    # 7 energy
    # 8 dspace
    # 9 10 intens   integr

    try:
        for line in f:
            i = i + 1
            if line.startswith("Grain no"):
                gnumloc = np.array((line.split())[2], dtype=int)
                print("gnumloc", gnumloc)
                if gnumloc == grain_index:
                    print("grain ", grain_index)
                    # print  " indexed peaks list starts at line : ", i
                    linestart = i + 1
                    grainfound = 1
            if grainfound == 1:
                # if i == linestart :
                # print line.rstrip("\n")
                if line.startswith("latticeparameters"):
                    # print "lattice parameters at line : ", i
                    dlatstr = np.array(line[18:].split(), dtype=float)
                    print("lattice parameters : \n", dlatstr)
                    # print "indexed peaks list ends at line = ", i
                    lineend = i - 1
                if line.startswith("dd,"):
                    # print "calib starts at line : ", i
                    calib[:3] = np.array(line[17:].split(), dtype=float)
                if line.startswith("xbet,"):
                    # print "calib line 2 at line = ", i
                    calib[3:] = np.array(line[11:].split(), dtype=float)
                if line.startswith("dev1,"):
                    dev_str = np.array(line.split()[3:], dtype=float)
                    print("deviations : \n", dev_str)
                if line.startswith("coordinates of a*"):
                    # print "matrix starts at line : ", i
                    linemat = i
                    grainfound = 0
    finally:
        linetot = i
        f.close()

    #    matstr = scipy.io.array_import.read_array(filestr, columns=(0, 1, 2),
    #                                              lines = (linemat, linemat + 1, linemat + 2))
    #    print "linemat", linemat
    matstr = np.genfromtxt(filename,
                            usecols=(0, 1, 2),
                            skip_header=linemat,
                            skip_footer=linetot - (linemat + 3))

    print("matstr", matstr)

    # print "linestart = ", linestart
    # print "lineend =", lineend
    # TODO: upgrade scipy.io.array_import to np.loadtxt
    #    data_str = scipy.io.array_import.read_array(filestr, columns=(0, (1, 11)), \
    #                                              lines = (linestart, (linestart + 1, lineend)))
    #    print "linestart", linestart
    #    print "linetot - lineend", linetot - lineend
    data_ = np.genfromtxt(filename,
                            dtype=None,
                            delimiter="\n",
                            names=True,
                            #                                 usecols=tuple(range(5)),
                            skip_header=linestart,
                            skip_footer=linetot - lineend)

    data_str = np.array([elem[0].split() for elem in data_], dtype=np.float32)[:, :11]

    print("number of indexed peaks :", len(data_str))
    print("first peak : ", data_str[0, 2:5])
    print("last peak : ", data_str[-1, 2:5])

    # print "return(data_str, satocrs, calib, dev_str)"
    # print "data_str :  xy(exp) 0:2 hkl 2:5 xydev 5:7 energy 7 dspacing 8  intens 9 integr 10"

    return data_str, matstr, calib, dev_str


def read_indexationfile(filename, grainindex_mat=0):
    r"""
    Read indexation file created by lauetools with extension .res

    Return arrays of all colums [spot# grainindex 2theta chi pixX pixY intensity h k l energy]

    .. todo:: adapt matrix reader to new file .res format

    """
    with open(filename, 'r') as f:
        while True:
            line = f.readline()
            if line.startswith('# Number of spots:'):
                nbspots = int(line.split(':')[1])
                break

    nb_lines_header = 4

    f = open(filename, 'r')
    for _ in range(nb_lines_header):
        f.readline()

    dat = np.fromfile(f, dtype=float, sep=" ")
    nbelems = len(dat)
    f.close()

    data_millerExy = dat.reshape((nbspots, nbelems//nbspots))

    spotindex = np.array(data_millerExy[:, 0])
    twicetheta = np.array(data_millerExy[:, 2])  # target values
    chi = np.array(data_millerExy[:, 3])  # target values
    pixX = np.array(data_millerExy[:, 4])
    pixY = np.array(data_millerExy[:, 5])
    Intensity = np.array(data_millerExy[:, 6])
    miller = np.array(data_millerExy[:, 7:10])
    energy = np.array(data_millerExy[:, 10])

    Grain_index = np.array(data_millerExy[:, 1])

    # removing spots that were in ambiguous positions to be indexed (energy set to 0)
    pos = np.where(energy != 10000.)[0]

    spotindex = np.take(spotindex, pos, axis=0)
    twicetheta = np.take(twicetheta, pos, axis=0)
    chi = np.take(chi, pos, axis=0)
    pixX = np.take(pixX, pos, axis=0)
    pixY = np.take(pixY, pos, axis=0)
    Intensity = np.take(Intensity, pos, axis=0)
    miller = np.take(miller, pos, axis=0)
    energy = np.take(energy, pos, axis=0)
    Grain_index = np.take(Grain_index, pos, axis=0)

    f = open(filename, "r")
    accum_mat = 0  # U matrix
    lineaccum = 0
    mat_index = 0
    datamat = {}

    accum_B = 0  # B matrix
    lineaccum_B = 0
    mat_index_B = 0
    datamat_B = {}

    accum_E = 0  # E Matrix (deviatoric strain)
    lineaccum_E = 0
    mat_index_E = 0
    datamat_E = {}

    accum_param = 0
    lineaccum_param = 0
    detector_param = {}

    for line in f.readlines():
        if accum_mat == 1 and lineaccum < 3:  # collect orientation matrix elements
            tru = line.split()[1:]
            datamat[grainindex_mat].append(tru)
            lineaccum += 1
        if accum_B == 1 and lineaccum_B < 3:  # collect Bmatrix elements
            tru = line.split()[1:]
            datamat_B[grainindex_mat].append(tru)
            lineaccum_B += 1
        if accum_E == 1 and lineaccum_E < 3:  # collect Ematrix elements
            tru = line.split()[1:]
            datamat_E[grainindex_mat].append(tru)
            lineaccum_E += 1
        if (accum_param == 1 and lineaccum_param < 5):  # collect calibration parameters elements
            tru = line.split()
            variable = tru[1]
            value = float(tru[3])
            detector_param[variable] = value
            lineaccum_param += 1

        if accum_mat == 1 and lineaccum == 3:
            accum_mat = 0
            mat_index += 1
        if accum_B == 1 and lineaccum_B == 3:
            accum_B = 0
            mat_index_B += 1
        if accum_E == 1 and lineaccum_E == 3:
            accum_E = 0
            mat_index_E += 1

        if line[:4] == "#Ori":
            datatype = "orientation"
        if line[:4] == "#BMa":
            datatype = "Bmatrix"
        if line[:4] == "#EMa":
            datatype = "Ematrix"
        if line[:2] == "#G":  # Grain matrix orientation start
            tra = line.split(" ")[1]
            # print "tra",tra
            grainindex_mat = tra.split(":")[0]
            # print "datatype",datatype
            if datatype == "orientation":
                accum_mat = 1
                lineaccum = 0
                datamat[grainindex_mat] = []
            if datatype == "Bmatrix":
                accum_B = 1
                lineaccum_B = 0
                datamat_B[grainindex_mat] = []
            if datatype == "Ematrix":
                accum_E = 1
                lineaccum_E = 0
                datamat_E[grainindex_mat] = []
        if line[:4] == "#Cal":  # calibration parameter start
            accum_param = 1
            lineaccum_param = 0

    f.close()

    for key, _ in datamat.items():
        datamat[key] = np.array(datamat[key], dtype=float)

    for key, _ in datamat_B.items():
        datamat_B[key] = np.array(datamat_B[key], dtype=float)

    for key, _ in datamat_E.items():
        datamat_E[key] = np.array(datamat_E[key], dtype=float)

    calib = []
    if detector_param:
        calib = []
        for key in ["dd", "xcen", "ycen", "xbet", "xgam"]:
            calib.append(detector_param[key])

    return (spotindex, twicetheta, chi, pixX, pixY, Intensity, miller, energy,
                Grain_index, datamat, datamat_B, datamat_E, calib)

def getpeaks_fromfit2d(filename):
    """
    read peaks list created by fit2d peak search

    #TODO: to remove function to read old data format, not used any longer
    """
    frou = open(filename, "r")
    alllines = frou.readlines()
    frou.close()
    peaklist = alllines[1:]

    print(" %d peaks in %s" % (len(peaklist), filename))
    outputfilename = filename[:-6] + ".pik"
    fric = open(outputfilename, "w")
    for line in peaklist:
        fric.write(line)
    fric.close()
    print("X,Y, int list in %s" % (outputfilename))
    return len(peaklist)

# ----------------------------------
# Lauetools .fit file parser
# rev. : 2016-08-03
# S. Tardif (samuel.tardif@gmail.com)
# --------------------------


class Peak:
    def __init__(self, p):
        if len(p) == 18:
            self.spot_index = float(p[0])
            self.Intensity = float(p[1])
            self.h = int(float(p[2]))
            self.k = int(float(p[3]))
            self.l = int(float(p[4]))
            self.pixDev = float(p[5])
            self.energy = float(p[6])
            self.Xexp = float(p[7])
            self.Yexp = float(p[8])
            self.twotheta_exp = float(p[9])
            self.chi_exp = float(p[10])
            self.Xtheo = float(p[11])
            self.Ytheo = float(p[12])
            self.twotheta_theo = float(p[13])
            self.chi_theo = float(p[14])
            self.Qx = float(p[15])
            self.Qy = float(p[16])
            self.Qz = float(p[17])
        elif len(p) == 12:
            self.spot_index = float(p[0])
            self.Intensity = float(p[1])
            self.h = int(float(p[2]))
            self.k = int(float(p[3]))
            self.l = int(float(p[4]))
            self.twotheta_exp = float(p[5])
            self.chi_exp = float(p[6])
            self.Xexp = float(p[7])
            self.Yexp = float(p[8])
            self.energy = float(p[9])
            self.GrainIndex = float(p[10])
            self.pixDev = float(p[11])


class LT_fitfile:
    """
    Parse the .fit file in a LT_fitfile object
    """

    # dictionary definitions for handling the LaueTools .fit files lines
    def __param__(self):
        return {
            "#UB matrix in q= (UB) B0 G* ": self.__UB__,
            "#B0 matrix in q= UB (B0) G*": self.__B0__,
            "#UBB0 matrix in q= (UB B0) G* i.e. recip. basis vectors are columns in LT frame: astar = UBB0[0,:], bstar = UBB0[1,:], cstar = UBB0[2,:]. (abcstar as lines on xyzlab1, xlab1 = ui, ui = unit vector along incident beam)": self.__UBB0__,
            "#UBB0 matrix in q= (UB B0) G* , abcstar as lines on xyzlab1, xlab1 = ui, ui = unit vector along incident beam : astar = UBB0[0,:], bstar = UBB0[1,:], cstar = UBB0[2,:]": self.__UBB0__,
            "#deviatoric strain in crystal frame (10-3 unit)": self.__devCrystal__,
            "#deviatoric strain in direct crystal frame (10-3 unit)": self.__devCrystal__,
            "#deviatoric strain in sample2 frame (10-3 unit)": self.__devSample__,
            "#DetectorParameters": self.__DetectorParameters__,
            "#pixelsize": self.__PixelSize__,
            "#Frame dimensions": self.__FrameDimension__,
            "#CCDLabel": self.__CCDLabel__,
            "#Element": self.__Element__,
            "#grainIndex": self.__GrainIndex__,
            "#spot_index intensity h k l 2theta Chi Xexp Yexp Energy GrainIndex PixDev": self.__Peaks__,
            "#spot_index Intensity h k l pixDev energy(keV) Xexp Yexp 2theta_exp chi_exp Xtheo Ytheo 2theta_theo chi_theo Qx Qy Qz": self.__Peaks__,
            "# Number of indexed spots": self.__NumberIndexedSpots__,
            "# Mean Deviation(pixel)": self.__MeanDev__,
        }

    def __UB__(self, f, l):
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ub11, ub12, ub13 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ub21, ub22, ub23 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ub31, ub32, ub33 = float(l[0]), float(l[1]), float(l[2])
        self.UB = np.array([[ub11, ub12, ub13], [ub21, ub22, ub23], [ub31, ub32, ub33]])

    def __B0__(self, f, l):
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        b011, b012, b013 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        b021, b022, b023 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        b031, b032, b033 = float(l[0]), float(l[1]), float(l[2])
        self.B0 = np.array([[b011, b012, b013], [b021, b022, b023], [b031, b032, b033]])

    def __UBB0__(self, f, l):
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ubb011, ubb012, ubb013 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ubb021, ubb022, ubb023 = float(l[0]), float(l[1]), float(l[2])
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ubb031, ubb032, ubb033 = float(l[0]), float(l[1]), float(l[2])
        self.UBB0 = np.array([[ubb011, ubb012, ubb013],
                            [ubb021, ubb022, ubb023],
                            [ubb031, ubb032, ubb033]])

    def __devCrystal__(self, f, l):
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep11, ep12, ep13 = float(l[0]) * 1e-3, float(l[1]) * 1e-3, float(l[2]) * 1e-3
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep22, ep23 = float(l[1]) * 1e-3, float(l[2]) * 1e-3
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep33 = float(l[2]) * 1e-3
        self.deviatoric = np.array([[ep11, ep12, ep13], [ep12, ep22, ep23], [ep13, ep23, ep33]])

    def __devSample__(self, f, l):
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep_sample11, ep_sample12, ep_sample13 = (float(l[0]) * 1e-3,
                                                float(l[1]) * 1e-3,
                                                float(l[2]) * 1e-3)
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep_sample22, ep_sample23 = float(l[1]) * 1e-3, float(l[2]) * 1e-3
        l = f.readline().replace("[", "").replace("]", "").replace("\n", "").split()
        ep_sample33 = float(l[2]) * 1e-3
        self.dev_sample = np.array([[ep_sample11, ep_sample12, ep_sample13],
                                    [ep_sample12, ep_sample22, ep_sample23],
                                    [ep_sample13, ep_sample23, ep_sample33]])

    def __DetectorParameters__(self, f, l):
        l = (f.readline()
            .replace("[", "")
            .replace("]", "")
            .replace("\n", "")
            .replace(" ", "")
            .split(","))
        self.dd = float(l[0])
        self.xcen = float(l[1])
        self.ycen = float(l[2])
        self.xbet = float(l[3])
        self.xgam = float(l[4])
        self.DetectorParameters = [self.dd, self.xcen, self.ycen, self.xbet, self.xgam]

    def __PixelSize__(self, f, l):
        l = (f.readline()
            .replace("[", "")
            .replace("]", "")
            .replace("\n", "")
            .replace(" ", "")
            .split(","))
        self.PixelSize = float(l[0])

    def __FrameDimension__(self, f, l):
        l = f.readline().replace("\n", "")
        if l[0] == "[":
            l = l.replace("[", "").replace("]", "").split("  ")
        elif l[0] == "(":
            l = l.replace("(", "").replace(")", "").split(", ")
        self.FrameDimension = [float(l[0]), float(l[1])]

    def __CCDLabel__(self, f, l):
        l = (f.readline()
            .replace("[", "")
            .replace("]", "")
            .replace("\n", "")
            .replace(" ", "")
            .split(","))
        self.CCDLabel = l[0]

    def __Element__(self, f, l):
        l = (f.readline()
            .replace("[", "")
            .replace("]", "")
            .replace("\n", "")
            .replace(" ", "")
            .split(","))
        self.Element = l[0]

    def __GrainIndex__(self, f, l):
        l = (f.readline()
            .replace("[", "")
            .replace("]", "")
            .replace("\n", "")
            .replace(" ", "")
            .split(","))
        self.GrainIndex = l[0]

    def __Peaks__(self, f, l):
        self.peak = {}
        for _ in list(range(self.NumberOfIndexedSpots)):
            l = f.readline().split()
            self.peak["{:d} {:d} {:d}".format(int(float(l[2])), int(float(l[3])), int(float(l[4])))] = Peak(l)

    def __NumberIndexedSpots__(self, _, l):
        self.NumberOfIndexedSpots = int(l.split(" ")[-1])

    def __MeanDev__(self, _, l):
        self.MeanDevPixel = float(l.split(" ")[-1])

    def __init__(self, filename, verbose=False):
        try:
            with open(filename, "rU") as f:
                self.filename = filename

                # read the header
                l = f.readline()
                self.corfile = l.split(" ")[-1]

                l = f.readline()
                self.timestamp, self.software = l.lstrip("# File created at ").split(" with ")

                # read the footer
                l = f.readline().replace("\n", "")
                while l != "\n" and l != "":
                    try:
                        self.__param__()[l](f, l)
                        if verbose:
                            print("read ", l)
                        l = f.readline().replace("\n", "")

                    except KeyError:
                        try:
                            # print l.split(':')[0]
                            self.__param__()[l.split(":")[0]](f, l)
                            l = f.readline().replace("\n", "")

                        except KeyError:
                            print("could not read line {}".format(l))
                            l = f.readline().replace("\n", "")

                # some extra calculations to get the direct and reciprocal lattice basis vector
                # NOTE: the scale of the lattice basis vector is UNKNOWN !!!
                #       they are given here with a arbitrary scale factor
                if not hasattr(self, "UBB0"):
                    self.UBB0 = np.dot(self.UB, self.B0)

                try:
                    self.astar_prime = self.UBB0[:, 0]
                    self.bstar_prime = self.UBB0[:, 1]
                    self.cstar_prime = self.UBB0[:, 2]

                    self.a_prime = np.cross(self.bstar_prime, self.cstar_prime
                    ) / np.dot(self.astar_prime, np.cross(self.bstar_prime, self.cstar_prime))
                    self.b_prime = np.cross(self.cstar_prime, self.astar_prime
                    ) / np.dot(self.bstar_prime, np.cross(self.cstar_prime, self.astar_prime))
                    self.c_prime = np.cross(self.astar_prime, self.bstar_prime
                    ) / np.dot(self.cstar_prime, np.cross(self.astar_prime, self.bstar_prime))

                    self.boa = np.linalg.linalg.norm(self.b_prime
                    ) / np.linalg.linalg.norm(self.a_prime)
                    self.coa = np.linalg.linalg.norm(self.c_prime
                    ) / np.linalg.linalg.norm(self.a_prime)

                    self.alpha = (np.arccos(np.dot(self.b_prime, self.c_prime)
                            / np.linalg.linalg.norm(self.b_prime)
                            / np.linalg.linalg.norm(self.c_prime)) * 180.0 / np.pi)
                    self.beta = (np.arccos(np.dot(self.c_prime, self.a_prime)
                            / np.linalg.linalg.norm(self.c_prime)
                            / np.linalg.linalg.norm(self.a_prime)) * 180.0 / np.pi)
                    self.gamma = (np.arccos(np.dot(self.a_prime, self.b_prime)
                            / np.linalg.linalg.norm(self.a_prime)
                            / np.linalg.linalg.norm(self.b_prime)) * 180.0 / np.pi)

                except ValueError:
                    print("could not compute the reciprocal space from the UBB0")

        except IOError:
            print("file {} not found! or problem of reading it!".format(filename))


if __name__ == "__main__":

    filepath = "checkubs.ubs"
    filepath = "SiHgCdTe.ubs"
    filepath = "/home/micha/LaueToolsPy3/LaueTools/Examples/CuSi/testUBS.ubs"
    res = readCheckOrientationsFile(filepath)
